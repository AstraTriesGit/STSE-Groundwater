---
title: "Examining The Combined GWL Dataset"
output: pdf_document
---

For instructions on how to get to this point, refer to `eda/combine_all_datasets.rmd`.
```{r}
library(feather)
library(tibble)

gwl <- read_feather("data/combined/gwl.feather")
glimpse(gwl)
```
Always handy to keep in mind the schema of the data.
Moving onto summary statistics.
```{r}
library(dplyr)

summary(select(gwl, latitude, longitude, data_time, data_value))
```
## `data_value`
This is the prediction target. The following need to be studied:
- analysis of observations with data_value = 0, it's no longer the realm of groundwater!
- confirmation whether positive values should be understood as abs(depth)
- what extent of readings should be considered as meaningful
- default values chosen by agencies to report _something_ instead of nothing (0/1/etc.)

To propose a query-aware filter, we have chosen (-200, -0.1) as the interval in which meaningful GWLs lie.
The filter will be applied after converting each value to -abs(value).
```{r}
# another possibility is using -160 as the bound
gwl_data_value_filtered <- gwl %>%
        mutate(data_value = -abs(data_value)) %>%
        filter((data_value > -200) & (data_value < -0.1))

remove(gwl) # required if computing power is limited
summary(gwl_data_value_filtered$data_value)

# gwl_ones <- gwl %>%
#   mutate(data_value = -abs(data_value)) %>%
#   filter(data_value == -1)
```
- (-10m, -0.1m) filter -> 11,997,074 observations
- (-15m, -0.1m) filter -> 14,675,859 observations
- (-200m, -0.1m) filter -> 22,018,553 observations

## `latitude` and `longitude`
One can apply a 'spatial filter' that removes observations whose coordinates do not fall within India.
The **shapefile quality** is essential to this process.
```{r}
library(ggplot2)
library(maps)

india_map <- map_data("world") %>%
  filter(region == "India")

roi <- ggplot() +
        geom_polygon(data = india_map,
                     aes(x = long, y = lat, group = group),
                     color = "black", alpha = 0, size = 0.1) +
        geom_point(data = gwl_data_value_filtered,
                   aes(x = longitude, y = latitude),
                   size = 0.15)

ggsave("plots/combined/roi.png", plot = roi, dpi = 150)
```
- Possible source of error: swapped lat/long coordinates?
Hence, we need a good shapefile to retain relevant values.
The current shapefile has been sourced from..............
```{r}
library(sf)
gwl_sf <- st_as_sf(gwl_data_value_filtered,
                   coords = c("longitude", "latitude"),
                   crs = 4326) # WGS84
india_region <- st_read("data/shp/IND_adm/IND_adm0.shp")

# the filter removed...~0.003% of observations!
# The number of observations removed doesn't quite justify the time the spatial filter takes
gwl_sf_region_filtered <- gwl_sf %>%
        filter(st_within(geometry, india_region, sparse = FALSE))

# 1749 init, still running as of 1910. This is much more painful than 11 million observations
# completed 1910! It only took 1h 10m!!
```

## Balanced Panels
We will now try and manufacture the biggest balanced panel we can make from our GWL data,
which by this point only contains valid coordinates and data values.
```{r}
library(lubridate)

# How many unique (lat, long, t) tuples do we have?
regular_df <- gwl_sf_region_filtered %>%
  mutate(
    longitude = st_coordinates(.)[,1],
    latitude = st_coordinates(.)[,2]
  ) %>%
  st_drop_geometry()

# I am saving regular_df as a sort of cache
# so that I don't have to engage in the EXTREMELY SLOW spatial filter operation
# write_feather(regular_df, "data/combined/gwl_post_filters.feather")
regular_df <- read_feather("data/combined/gwl_post_filters.feather")

# time values rounded to the nearest month, and also get count made for each (long, lat, t)
unique_points <- regular_df %>%
  mutate(data_time = round_date(data_time, unit = "month")) %>%
  group_by(longitude, latitude, data_time) %>%
  summarise(count = n(),
            avg_gwl = mean(data_value),
            cv_gwl = sd(data_value)/mean(data_value)
  )
```

### Coefficient of Variation
We have rounded each timestamp to the nearest month.
Additionally, we have aggregated the observations made at each st-location
by taking their average. The validity of this operation depends on the
values of the CVs for each st-location.
```{r}
library(ggplot2)

# suspect that NAs are introduced by groups having only one observation
# sd() returns NA for only one observation
unique_points <- unique_points %>%
        mutate(cv_gwl = if_else(is.na(cv_gwl), 0, cv_gwl))

plot_cv <- ggplot(unique_points, aes(x = seq_along(cv_gwl), y = cv_gwl)) +
        labs(y = "Coefficient of Variation", x = "Index", title = "Coefficient of Variation in Observations across Spatio-temporal Locations") +
        geom_point(size = 0.5)
ggsave("plots/combined/cv_plot.png", plot = plot_cv, dpi = 150)

summary(unique_points$cv_gwl)
count_cv_less_than_1 <- nrow(filter(unique_points, cv_gwl > -1))
(nrow(unique_points) - count_cv_less_than_1)
```
99.67% of st-locations have a CV value of less than 1. Applying this filter
will result in the loss of 3614 st-locations.
Conclude what you want to do with this information.

### How Sparse is the Time Axis?
Some metric of completeness is required.
```{r}
unique_locations <- unique_points %>%
        group_by(latitude, longitude) %>%
        summarise(n_obs = n())

summary(unique_locations$n_obs)
```
Evidently, we have a VERY sparse t-axis. 10 years of data and 12 months
means that in an ideal scenario, we would have 120 observations from each unique
spatial location. The median number of observations is...11!

```{r}
observations_per_t <- unique_points %>%
        group_by(data_time) %>%
        summarise(count = n())

summary(observations_per_t$count)
```

Let's get ourselves a plot for the same.
```{r}
ggplot(observations_per_t, aes(y = count, x = data_time)) +
        geom_col() +
        geom_text(aes(label = round(count/1e3)), vjust = -0.5) +
        labs(y = "No. of Observations", x = "Timestamp")

ggsave("plots/combined/observations_per_t.png", dpi = 150)
```
How tf are we supposed to have ANY kind of time series BS with this??


```{r}
observations_per_month <- observations_per_t %>%
        mutate(month_ = month(data_time)) %>%
        group_by(month_) %>%
        summarise(count_ = sum(count),
                  mean_count = count_ / 10,
                  median_count = median(count),
                  sd_count = sd(count),
                  cv_count = sd_count / median_count
        ) %>%
        arrange(mean_count)

glimpse(observations_per_month)
```

Time to pray to god.
```{r}
gwl_may <- unique_points %>%
        filter(month(data_time) == 5)

unique_locations_in_may <- gwl_may %>%
        group_by(longitude, latitude) %>%
        summarise(count = n())

may_observations_per_year <- gwl_may %>%
        mutate(year_ = year(data_time)) %>%
        group_by(year_) %>%
        summarise(count = n())

may_2014 <- gwl_may %>%
        filter(year(data_time) == 2014)

may_2015 <- gwl_may %>%
        filter(year(data_time) == 2015)

common <- inner_join(common,
                     gwl_may %>%
                             filter(year(data_time) == 2018),
                     by = c("latitude", "longitude"))

summary(unique_locations_in_may$count)
```
There are no gods in the world of econometrics.

### Balanced Panels: The station_code Attempt
If (latitude, longitude) pairs are going to be the source of conflict, might as well work
with the `station_code` fields in the original data. Why?-Why not?
```{r}
unique_stcodes <- regular_df %>%
  distinct(station_code)
# 73k unique station_codes! In comparison to 97k lat-long pairs! A step in the right direction.
# 1.1 mil unique (station_code, t) st-locations. In comparison to 1 million (lat, long, t) st-locations

unique_points_stcodes <- regular_df %>%
  mutate(data_time = round_date(data_time, unit = "month")) %>%
  group_by(station_code, data_time) %>%
  summarise(count = n(),
            avg_gwl = mean(data_value),
            cv_gwl = sd(data_value)/mean(data_value))

summary(select(unique_points_stcodes, count, cv_gwl))
```

It's already better than the lat-long pairs! Should we continue creating our panels with
our new primary key?
```{r}
gwl_may <- unique_points_stcodes %>%
  filter(month(data_time) == 5)

unique_locations_in_may <- gwl_may %>%
  group_by(station_code) %>%
  summarise(count = n())

may_observations_per_year <- gwl_may %>%
  mutate(year_ = year(data_time)) %>%
  group_by(year_) %>%
  summarise(count = n())

may_2014 <- gwl_may %>%
  filter(year(data_time) == 2018)

may_2015 <- gwl_may %>%
  filter(year(data_time) == 2019)

common <- inner_join(may_2015, may_2014, by = "station_code")
common <- inner_join(common,
                     gwl_may %>%
                       filter(year(data_time) == 2022),
                     by = "station_code")

```

This shit is so ass. Something changed up between 2014-18's and 2018-2023's indexing.





```{r}
# My honest reaction when grouping creates the same amount of rows as original df.
# Should I skip calculating those statistics? They won't really add much information.
# On second thought I think I might just let it run.
# Operation started at 1232, how long will it take? No one knows, not even the computer!
unique_locations_stcode <- regular_df %>%
  group_by(station_code, data_time) %>%
  summarise(count = n(),
            avg_gwl = mean(data_value),
            cv_gwl = sd(data_value)/mean(data_value)
  )

# checkpoint 2: unique_locations_stcode. It takes 30 mins to do aforementioned group_by.
# write_feather(unique_locations_stcode, "data/combined/gwl_by_stcode.feather")
# unique_locations_stcode <- read_feather("data/combined/gwl_by_stcode.feather")
```
A consideration: the number of unique `station_code` fields is less than that number of unique lat-long
pairs. Using the pigeonhole principle, we conclude that some station_codes map to more than one
lat-long pair. Hence, a function mapping station_codes to lat-long pairs is not a bijection. And that
is a problem. How do we plan our database joins for such a case?

## Data Engineering Assumptions Made So Far (2/10)
- the India map in the shapefile is a suitable filter for our data
- rounding to the nearest month is valid (needs backing of coefficient of variation)
- we can continue an analysis without a balanced panel (o rly?)